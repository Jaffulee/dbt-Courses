C2-L5-modern data stack explainer

All right. Let's focus on dbt in the modern data stack. Let's see how everything fits together. First, we have a ton of data sources these days.

This could be Salesforce data, HubSpot data, all sorts of data being collected about our business, which we'll use to make key decisions. At the center of the modern data stack is your data platform. This could be Snowflake, Redshift, BigQuery, Databricks, or a plethora of other tools that are available for storing your data as the central location for your business. Those four are supported in dbt cloud, but if using dbt core, there are a ton of different community supported adapters for different data.

And so you may be wondering how does this data move from data sources to data platforms? You could transfer all this data with custom tooling, through Python, Scala, or Java, but there's an emergence of a ton of different tools known as loaders or EL tools that will extract this data from those sources and then load them into your data platform. These are the E L step of the ELT framework as we move to cloud data platforms. And so once you have that data in your data platform, you probably want to use that data for something that's where BI tools come in.

ML models come in. As well as operational analytics, where we may take some curated data from our data platform and push that back to our applications. And so let's draw some arrows here. We may be pushing the data to Tableau Looker mode, power BI, or maybe we're pushing it to an ML model or a notebook that we're using maybe in Hex or something.

Or taking that data and pushing it into Census or Hightouch to then use that data really productively in one of our applications. So where does dbt fit in? dbt works directly with your data platform to manage your transformations, test them, and then also document your transformations along the way. So let's zoom in on this interaction here between dbt and your data platform.

From on the left, we see raw data coming in and at the end of this process, we have curated data sets that are ready for our BI tools, ML models, and operational analytics. How do we get there though? With dbt it's really easy to develop your transformation pipeline because you are writing modular code in models as SQL select statements. You don't need to worry about the DDL and the DML to wrap around that.

You're just focusing on select statements. And while you write those models you are building dependencies between models to transform that data over time. And so if we want to visualize that, we see, we have this dbt DAG for data lineage: directed acyclic graph. So on the left, we have our sources and those are transformed into our staging layer here into multiple models downstream.

And then we can even make dbt aware of the BI layer or any of those downstream workflows. So while you're writing dbt code, you are slowly assembling this DAG. So you see complete lineage from source through use case there. At the end, we come back up here as you're developing your models, you can also test your models.

So you can ensure that a primary key is in fact unique or is actually not null before shipping that data and then having that run in production. While you're also configuring tests. You can also document your models while you write the transformations right in the same code base. And so you create a new model, you configure tests on it, and then you write documentation while that is fresh in your mind.

You don't need to open a new tab, go into a different system. It's all right there. So once you agree and trust your transformations and you've tested them and you've documented them, you can then deploy your dbt projecton a schedule. Through the dbt cloud interface is really easy to do.

You set up a production environment or deployment environment, and then you set up a job. You can run that weekly, daily, you can re really get aggressive and run that hourly. If that is the use case you need. Then you have these refreshed datasets on whatever cadence that you need.

So if we step back, we see that DBT is the T and the ELT framework in our modern data stack, powering the transformations of that raw data through those curated datasets to power your BI tools, ML models, and your operational analytics workloads.
