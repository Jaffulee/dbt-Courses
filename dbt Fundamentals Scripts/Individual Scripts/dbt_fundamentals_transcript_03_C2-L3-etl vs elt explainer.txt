C2-L3-etl vs elt explainer

So you've been working in data for awhile. You've likely done the following extracted data from some source, whether it's email as a CSV or an Excel file, manipulated that data and then serve that data over to a colleague so that they can make decisions based off your analysis. This is in a nutshell, like its own ETL process. You're extracting the data by downloading it.

You're manipulating that data. You're transforming it and then you're loading that data, giving it back to someone who can use it in that new state. The more traditional ETL process is handled by data engineers, where the data is taken out of a database, transformed on some third party machine and then loaded back into the database so that data analysts can query that data. This usually requires a few, additional skills and on top of SQL Python, Java, other functional programming languages to make this happen.

Then in addition to that, once that table is built, that process of building that table, refreshing that table, inserting new records, rebuilding the table, whatever it is, it needs to be automated, and that requires additional tooling, tools like airflow have commonly been used to automate that process. But then there's been this recent addition and introduction of data warehouses that are based in the cloud. And because they're based in the cloud, organizations can just purchase them and scale them up as needed. They don't need to build these things on premise, so they just have them when they need it.

And then they'll also this term data warehouse has emerged and it means slightly different things at different organizations, but in a nutshell, the data warehouse is the coupling of a database and a supercomputer that can run transformations run code against that database. This has been a complete game changer for the analytics workflow. Particularly, because you can just take raw data, get that into your data warehouse and then transform it from there. There's no longer this extract load extract load process every time you want to rebuild new database object.

And so this has changed the term ETL, swapped it around a little bit to E L T extract load transform. So for data teams, what this means is we can first focus on just the E and the L. Let's extract data from some source and then load it into our data warehouse, just get it in the data warehouse. And then once it's there, we can transform it into whatever shape we need so that analysts can query that data later.

Total game changer. And some other impacts of this are the compute that's associated with data warehouses is scalable. If you need a more powerful computer, you can get that just by asking your provider for more. You have to pay for it, but you can just get a stronger computer.

Also as your organization grows, you're inevitably going to need more space to store that data. In the cloud, these databases, data warehouses are scalable. You can store more data as you need. And then finally, there's this old process of extracting data and then loading it back every time you want to build a new database object.

That doesn't happen anymore. Raw data is there. You transform it? There's no longer this transfer of data, consuming a lot of energy and probably consuming a lot of financial resources to make that happen.

And so with this game changing technology, we can now introduce this new role of analytics engineer and rethink about how our data teams work together. That's what we want to cover next.
